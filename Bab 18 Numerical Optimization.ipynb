{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "64a206ed-5880-4ed8-86fe-43355a15090b",
   "metadata": {},
   "source": [
    "# Algorithm 18.1 (Local SQP Algorithm for solving (18.1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b384ac49-e93b-4d7d-9301-c4919b9b4f2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "def objective(x):\n",
    "    \"\"\" Define the objective function f_k. \"\"\"\n",
    "    return np.sum(x**2)  # Example: Quadratic function\n",
    "\n",
    "def gradient(x):\n",
    "    \"\"\" Compute the gradient \\nabla f_k. \"\"\"\n",
    "    return 2 * x\n",
    "\n",
    "def hessian(x):\n",
    "    \"\"\" Compute the Hessian \\nabla^2 L_k. \"\"\"\n",
    "    return 2 * np.eye(len(x))\n",
    "\n",
    "def constraints(x):\n",
    "    \"\"\" Define the constraint functions c_k. \"\"\"\n",
    "    return np.array([np.sum(x) - 1])  # Example: sum(x) = 1\n",
    "\n",
    "def jacobian(x):\n",
    "    \"\"\" Compute the Jacobian A_k. \"\"\"\n",
    "    return np.ones((1, len(x)))  # Derivative of sum(x)\n",
    "\n",
    "def solve_qp(hess, grad, A, c):\n",
    "    \"\"\" Solve the Quadratic Programming subproblem (18.7). \"\"\"\n",
    "    def qp_objective(p):\n",
    "        return 0.5 * p @ hess @ p + grad @ p\n",
    "    \n",
    "    def qp_constraint(p):\n",
    "        return A @ p + c\n",
    "    \n",
    "    constraints = { 'type': 'eq', 'fun': qp_constraint }\n",
    "    result = minimize(qp_objective, np.zeros_like(grad), constraints=constraints)\n",
    "    return result.x if result.success else None\n",
    "\n",
    "def local_sqp(x0, lambda0, tol=1e-6, max_iter=100):\n",
    "    x = x0\n",
    "    lambda_ = lambda0\n",
    "    k = 0\n",
    "    \n",
    "    while k < max_iter:\n",
    "        f_k = objective(x)\n",
    "        grad_f_k = gradient(x)\n",
    "        hess_L_k = hessian(x)\n",
    "        c_k = constraints(x)\n",
    "        A_k = jacobian(x)\n",
    "        \n",
    "        p_k = solve_qp(hess_L_k, grad_f_k, A_k, c_k)\n",
    "        if p_k is None or np.linalg.norm(p_k) < tol:\n",
    "            break\n",
    "        \n",
    "        x = x + p_k\n",
    "        lambda_ = -np.linalg.pinv(A_k @ A_k.T) @ A_k @ grad_f_k  # Lagrange multiplier update\n",
    "        k += 1\n",
    "    \n",
    "    return x, lambda_\n",
    "\n",
    "# Example usage\n",
    "x0 = np.array([0.5, 0.5])  # Initial guess for x\n",
    "lambda0 = np.array([0.0])  # Initial guess for lambda\n",
    "solution, lambda_final = local_sqp(x0, lambda0)\n",
    "print(\"Optimal x:\", solution)\n",
    "print(\"Optimal lambda:\", lambda_final)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "685ab927-6044-4606-8d6b-c24afe59f0c7",
   "metadata": {},
   "source": [
    "# Algorithm 16.3 (Line Search SQP Algorithm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18a1d253-bb79-4ecd-a039-ad76ef2b3a8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def line_search_sqp(f, grad_f, c, A, x0, lambda0, eta=0.3, tau=0.5, tol=1e-6, max_iter=100):\n",
    "    n = len(x0)\n",
    "    m = len(lambda0)\n",
    "    x_k = np.array(x0, dtype=float)\n",
    "    lambda_k = np.array(lambda0, dtype=float)\n",
    "    B_k = np.eye(n)  # Initial Hessian approximation if quasi-Newton is used\n",
    "    \n",
    "    def merit_function(x, mu):\n",
    "        return f(x) + mu * np.linalg.norm(c(x), 1)\n",
    "    \n",
    "    for _ in range(max_iter):\n",
    "        # Compute search direction p_k by solving the QP subproblem (18.11)\n",
    "        grad_L = grad_f(x_k) - A(x_k).T @ lambda_k  # Gradient of Lagrangian\n",
    "        p_k = -np.linalg.solve(B_k, grad_L)  # Newton step (approximation)\n",
    "        lambda_hat = np.linalg.solve(A(x_k) @ A(x_k).T, A(x_k) @ grad_f(x_k))\n",
    "        p_lambda = lambda_hat - lambda_k\n",
    "        \n",
    "        # Line search\n",
    "        mu_k = 1  # Initial choice\n",
    "        alpha_k = 1\n",
    "        while merit_function(x_k + alpha_k * p_k, mu_k) > \\\n",
    "              merit_function(x_k, mu_k) + eta * alpha_k * np.dot(grad_L, p_k):\n",
    "            alpha_k *= tau  # Reduce step size\n",
    "        \n",
    "        # Update iterates\n",
    "        x_k += alpha_k * p_k\n",
    "        lambda_k += alpha_k * p_lambda\n",
    "        \n",
    "        # Convergence check\n",
    "        if np.linalg.norm(p_k) < tol:\n",
    "            break\n",
    "        \n",
    "        # Quasi-Newton update\n",
    "        s_k = alpha_k * p_k\n",
    "        y_k = grad_f(x_k) - grad_f(x_k - s_k)\n",
    "        if np.dot(s_k, y_k) > 0:\n",
    "            B_k += np.outer(y_k, y_k) / np.dot(y_k, s_k) - np.outer(B_k @ s_k, B_k @ s_k) / np.dot(s_k, B_k @ s_k)\n",
    "    \n",
    "    return x_k, lambda_k\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0146909c-c982-45c5-8ad3-0de3e022907e",
   "metadata": {},
   "source": [
    "# Algoritma 16.4 (Byrd-Omojokun Trust-Region SQP Method) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1a7922b5-f28b-49ee-9b86-d106a851626c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_f(x):\n",
    "    # Define the objective function f(x)\n",
    "    pass\n",
    "\n",
    "def compute_c(x):\n",
    "    # Define the constraint function c(x)\n",
    "    pass\n",
    "\n",
    "def compute_grad_f(x):\n",
    "    # Compute the gradient of f\n",
    "    pass\n",
    "\n",
    "def compute_A(x):\n",
    "    # Compute the Jacobian of the constraints\n",
    "    pass\n",
    "\n",
    "def compute_multiplier_estimates(A, grad_f):\n",
    "    # Compute Lagrange multiplier estimates\n",
    "    return np.linalg.solve(A @ A.T, A @ grad_f)\n",
    "\n",
    "def solve_subproblem(v_k, r_k):\n",
    "    # Solve the subproblem to get v_k and compute r_k\n",
    "    pass\n",
    "\n",
    "def compute_hessian_or_quasi_newton(x):\n",
    "    # Compute Hessian \\nabla^2_x L or a quasi-Newton approximation\n",
    "    pass\n",
    "\n",
    "def projected_CG_method(H, grad_f, A):\n",
    "    # Compute search direction p_k using Projected Conjugate Gradient method\n",
    "    pass\n",
    "\n",
    "def choose_mu():\n",
    "    # Choose regularization parameter \\mu_k\n",
    "    pass\n",
    "\n",
    "def compute_ratio(ared, pred):\n",
    "    # Compute rho_k = ared_k / pred_k\n",
    "    return ared / pred if pred != 0 else 0\n",
    "\n",
    "def byrd_omojokun_trust_region_sqp(x0, delta0, epsilon, gamma, eta, max_iters=100):\n",
    "    x_k = x0\n",
    "    delta_k = delta0\n",
    "    \n",
    "    for k in range(max_iters):\n",
    "        f_k = compute_f(x_k)\n",
    "        c_k = compute_c(x_k)\n",
    "        grad_f_k = compute_grad_f(x_k)\n",
    "        A_k = compute_A(x_k)\n",
    "        \n",
    "        lambda_k = compute_multiplier_estimates(A_k, grad_f_k)\n",
    "        \n",
    "        if np.linalg.norm(grad_f_k - A_k.T @ lambda_k, np.inf) < epsilon and np.linalg.norm(c_k, np.inf) < epsilon:\n",
    "            return x_k  # Stop with approximate solution\n",
    "        \n",
    "        v_k, r_k = solve_subproblem(None, None)  # Placeholder\n",
    "        H_k = compute_hessian_or_quasi_newton(x_k)\n",
    "        p_k = projected_CG_method(H_k, grad_f_k, A_k)\n",
    "        mu_k = choose_mu()\n",
    "        \n",
    "        # Compute actual and predicted reduction (placeholders for now)\n",
    "        ared_k, pred_k = 1, 1  # Replace with proper computations\n",
    "        rho_k = compute_ratio(ared_k, pred_k)\n",
    "        \n",
    "        if rho_k > eta:\n",
    "            x_k = x_k + p_k\n",
    "            delta_k = max(delta_k, delta_k)  # Ensure Delta_{k+1} >= Delta_k\n",
    "        else:\n",
    "            delta_k = min(delta_k, gamma * np.linalg.norm(p_k))\n",
    "        \n",
    "    return x_k\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a433ee24-c57a-49a1-ba87-3f9ca0fd9507",
   "metadata": {},
   "source": [
    "# Algorithm 18.5 (Penalty Update and Step Computation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d69fe90d-20a1-45fb-a745-72536bc93a16",
   "metadata": {},
   "outputs": [],
   "source": [
    "def penalty_update(x_k, mu_k_minus_1, Delta_k, epsilon_1, epsilon_2, m_k, p, q):\n",
    "    \"\"\"\n",
    "    Implements Algorithm 18.5 (Penalty Update and Step Computation).\n",
    "    \n",
    "    Parameters:\n",
    "    x_k : current iterate (not used in this function but typically relevant in broader optimization context)\n",
    "    mu_k_minus_1 : previous penalty parameter\n",
    "    Delta_k : trust-region radius (not directly used here)\n",
    "    epsilon_1, epsilon_2 : algorithm parameters (0,1)\n",
    "    m_k : function m_k(y) (model function at iteration k)\n",
    "    p : function p(mu) solving the subproblem\n",
    "    q : function q(y) (another function required for the final condition)\n",
    "    \n",
    "    Returns:\n",
    "    mu_k : updated penalty parameter\n",
    "    p_k : updated step computation\n",
    "    \"\"\"\n",
    "    # Solve subproblem to get p(mu_k_minus_1)\n",
    "    p_mu_k_minus_1 = p(mu_k_minus_1)\n",
    "    \n",
    "    if m_k(p_mu_k_minus_1) == 0:\n",
    "        mu_plus = mu_k_minus_1\n",
    "    else:\n",
    "        # Compute p_infinity\n",
    "        p_infinity = p(float('inf'))  # Assuming p(∞) can be approximated numerically\n",
    "        \n",
    "        if m_k(p_infinity) == 0:\n",
    "            # Find mu^+ > mu_k_minus_1 such that m_k(p(mu^+)) = 0\n",
    "            mu_plus = mu_k_minus_1 * 1.1  # Increase mu slightly (heuristic choice)\n",
    "            while m_k(p(mu_plus)) != 0:\n",
    "                mu_plus *= 1.1  # Increase mu until condition is met\n",
    "        else:\n",
    "            # Find mu^+ ≥ mu_k_minus_1 such that m_k(0) - m_k(p(mu^+)) ≥ ε₁ [m_k(0) - m_k(p_infinity)]\n",
    "            mu_plus = mu_k_minus_1 * 1.1\n",
    "            while m_k(0) - m_k(p(mu_plus)) < epsilon_1 * (m_k(0) - m_k(p_infinity)):\n",
    "                mu_plus *= 1.1\n",
    "    \n",
    "    # Increase mu^+ if necessary to satisfy q condition\n",
    "    while q(0) - q(p(mu_plus)) < epsilon_2 * mu_plus * (m_k(0) - m_k(p(mu_plus))):\n",
    "        mu_plus *= 1.1\n",
    "    \n",
    "    # Update values\n",
    "    mu_k = mu_plus\n",
    "    p_k = p(mu_plus)\n",
    "    \n",
    "    return mu_k, p_k\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
